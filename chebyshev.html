<!DOCTYPE html>
<html>
<head>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta charset="UTF-8">
  <style>
  * {
    box-sizing: border-box;
  }

  html {
    font-family: monospace;
    font-family: cursive;
    font-family: serif;
  }
  
  p {
    font-size: 14pt;
    /* text-align: justify; */
    /* text-justify: inter-word; */
  }

  /* Create three equal columns that floats next to each other */
  .column-content {
    float: left;
    width: 50%;
    padding: 10px;
  }

  .column-side {
    float: left;
    width: 25%;
    padding: 10px;
  }

  /* p {
    line-height: 130%;
  } */

  /* Clear floats after the columns */
  .row:after {
    content: "";
    display: table;
    clear: both;
  }

  blockquote p {
    text-align: justify;
    font-size: 12pt;
  }

  /* IMPORTANT!
   * This is a quick fix in order to left-align your equations. Because
   * linebreaks does not work in version 3.2.
   * see
   * https://github.com/mathjax/MathJax/issues/2312
   */
  mjx-mtd {
    display: table-cell;
    text-align: left !important;
    padding: .215em .4em;
  }

  /**
   * REMOVES underlining for links
   */
  a:link { text-decoration: none; }
  a:visited { text-decoration: none; }
  a:hover { text-decoration: none; }
  a:active { text-decoration: none; }
  </style>
  <!-- https://www.mathjax.org/MathJax-v3.2.0-available/ -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> 
  <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-chtml.js"></script>
  

  <script>
    /***********************************************************************
     * MATHJAX CONFIGURATION                                               *
     ***********************************************************************/
    // http://docs.mathjax.org/en/latest/options/input/tex.html
    window.MathJax = {
      loader: {
        // http://docs.mathjax.org/en/latest/input/tex/extensions/tagformat.html
        load: ['[tex]/tagformat']
      },
      tex: {
        inlineMath: [['$', '$']],
        // https://docs.mathjax.org/en/v3.2-latest/input/tex/eqnumbers.html
        tags: 'ams',
        packages: {'[+]': ['tagformat']},
        tagformat: {
          number: (n) => n.toString(),
          tag:    (tag) => '(' + tag + ')',
          id:     (id) => '' + id.replace(/\s/g, '_'),
          url:    (id, base) => base + '#' + encodeURIComponent(id),
        }
      },
    }
  </script>
  </head>
  <!-- style.zoom is a way to be responsive...  -->
  <body> 
  <!-- https://jinja.palletsprojects.com/en/3.0.x/templates/#base-template -->
  <div class="row">
    <div class="column-side">
    </div>
    <div class="column-content">
      <p>$$
\newcommand{\bb}[1]{\mathbb{#1}}
\newcommand{\cc}[1]{\mathcal{#1}}
\newcommand{\txt}[1]{\;\;\textrm{#1}\;\;}
\newcommand{\problemUnconstr}{\min f(x)\; \txt{s.t.}\; x \in \bb{R}^n}
\newcommand{\problemMinimizeSingle}[2]{\min #1\; \txt{s.t.}\; #2 }
\newcommand{\problemMinimizeMulti}[2]{
  \displaylines{
    \min #1 \newline
    \txt{s.t.} #2
  }
}
$$</p>
<h1 id="numerical-solution-of-function-approximations-with-semi-infinite-programming">Numerical solution of function approximations with semi-infinite programming</h1>
<blockquote>
<blockquote>
<p>ABSTRACT: In this document a Chebyshev's approximation to a real valued function is performed through a semi-infinite programming problem. This reformulation uses the software tools available for optimization problems to compute the approximation. In particular, the computer program employed to execute the optimization task relies heavily in Sequential Quadratic Programming (SQP) method. In order to made this document self-contained, the definitions and techniques that composes the SQP method are described. In the first section, the problem restatement into semi-infinite programming terms is detailed, and some problem examples are portrayed. The following section describes the SQP techniques and core concepts that makes the method. In the final section, the sample problems shown are computed.<br>
KEYWORDS: chebyshev's aproximation, semi-infinite programming, sequential quadratic programming, constrained optimization.</p>
</blockquote>
</blockquote>
<h2 id="table-of-contents">Table of Contents</h2>
<div class="toc">
<ul>
<li><a href="#numerical-solution-of-function-approximations-with-semi-infinite-programming">Numerical solution of function approximations with semi-infinite programming</a><ul>
<li><a href="#table-of-contents">Table of Contents</a></li>
<li><a href="#1-problems-definition">1. Problem's definition</a><ul>
<li><a href="#11-chebyshev-approximation-problem-cap">1.1 Chebyshev approximation problem (CAP)</a></li>
<li><a href="#12-semi-infinite-programming-problem-sip">1.2 Semi-infinite programming problem (SIP)</a></li>
<li><a href="#13-cap-in-terms-of-sip">1.3 CAP in terms of SIP</a></li>
<li><a href="#14-approximation-function-definition">1.4 Approximation function definition</a></li>
<li><a href="#15-problem-model-specification">1.5 Problem model specification</a></li>
<li><a href="#16-problem-instances">1.6 Problem instances</a></li>
</ul>
</li>
<li><a href="#2-sqp-method">2. SQP Method</a><ul>
<li><a href="#21-newtons-method">2.1 Newton's Method</a></li>
<li><a href="#22-lagrange-multipliers">2.2 Lagrange Multipliers</a></li>
<li><a href="#23-kkt-optimality-conditions">2.3 KKT optimality conditions</a></li>
<li><a href="#24-quadratic-programming">2.4 Quadratic programming</a></li>
<li><a href="#25-sequential-quadratic-programming-method-sqp">2.5 Sequential Quadratic Programming method SQP</a></li>
</ul>
</li>
<li><a href="#3-examples-computation">3. Examples computation</a></li>
<li><a href="#4-source-code-revision">4. Source code revision</a></li>
<li><a href="#todo">TODO</a></li>
<li><a href="#referencias">Referencias</a></li>
</ul>
</li>
</ul>
</div>
<h2 id="1-problems-definition">1. Problem's definition</h2>
<p>In this section we will provide the necessesary definitions in order to put the <em>Chebyshev's approximation problem</em> (CAP) in terms of a <em>semi-infinite programming problem</em> (SIP).</p>
<h3 id="11-chebyshev-approximation-problem-cap">1.1 Chebyshev approximation problem <em>(CAP)</em></h3>
<p>The <em>Chebyshev approximation problem</em> <em>(CAP)</em> can be formulated in serveral ways, one of them is described as the following <em>minimax</em> problem:
$$
\begin{equation}
  \min_{ x \in K_{n-1}} \max_{w \in \Omega} |d(w) - F(x, w)|
  \label{chebyshevproblem}
\end{equation}
$$
where 
  $ K_{n-1} \subseteq \mathbb{R}^{n-1} $ and
  $ \Omega \subseteq \mathbb{R}^{m} $ are non-empty and compact sets, 
  $ d: \Omega \mapsto \mathbb{R} $ and
  $ F: K_{n-1}\times\Omega \mapsto \mathbb{R} $ are smooth functions given as input to the problem. Here, $ d(w) $ and $ F(x, w) $ represents the function to aproximate and the approximation function respectively, where $ x $ is the vector of parameters or coefficients that we want to optimize. Note that the approximation error given by $ |d(w) - F(x, w)| $ is not squared and computed linearly.</p>
<h3 id="12-semi-infinite-programming-problem-sip">1.2 Semi-infinite programming problem <em>(SIP)</em></h3>
<p>In general terms, a SIP is an optimization problem described as follows:</p>
<p>$$
\begin{equation}
\displaylines{
  \min \; f(x) \newline
  \textrm{s.t.} \; g(x, w) \leq 0, \; x \in \bb{R}, \; w \in \Omega, \; |\Omega| = \infty 
}
\label{def-sip}
\end{equation}
$$</p>
<p>where 
  $ f: K \mapsto \bb{R} $ and
  $ g: K \times \Omega \mapsto \bb{R} $ are smooth functions ($ g $ will be referred as semi-infinite constraint), with
  $ \Omega \subseteq \bb{R}^{n-1} $, 
  $ K \subseteq \bb{R}^{m} $. In general, this problem definition can have other kind constraints, i.e. equality, inequality and several semi-infinite constraints, but at least must be one semi-infinite constraint to have a SIP.</p>
<p>In other words, a SIP is just a minimization problem where one of the constraints is parametrized with a variable that belongs to an infinite set, leaving virtually an infinite number of constrains (one for each possible parameter value).</p>
<p>Also, the problem $ \eqref{def-sip} $ can be reformulated as follows:
$$
  \begin{equation}
  \displaylines{
    \min \;\; h(y) := y \newline
    \textrm{s.t.} \quad g(x, w) \leq 0, \newline
    \qquad \; f(x) \leq y
  }
  \label{def-sip2}
  \end{equation}
$$
where $ g, f, x, w, K, \Omega $ have the same types as in $\eqref{def-sip}$ and $ y \in K $. This new reformulation is equivalent since once the constraint $ g(x, w) $ is satisfied, now the least value is imposed by $ f(x) $, as in the original problem.</p>
<h3 id="13-cap-in-terms-of-sip">1.3 CAP in terms of SIP</h3>
<p>The following example can help to understand the <em>reformulation technique</em> [6:307] employed at the end of this subsection that will allow to pose CAP in terms of SIP. In the following example, this reformulation removes the non-differentiable points at $ x = 0 $ and $ x = 1 $:
$$
\begin{equation}
\problemMinimizeSingle{\max(x^2, x)}{x \in \bb{R}}
\label{capsipex1}
\end{equation}
$$
The reformulation consist only in adding the artificial variable $ t $:
$$
\problemMinimizeSingle{t \in \bb{R}}{t \geq x,\; t \geq x^2,\; x \in \bb{R}}
$$
The trick happens when minimization of $ t $ is done from above for $ x $ and $ x^2 $ i.e. $ t \geq x $ and $ t \geq x^2 $. If $ t_i^\star $ denotes a feasible $ t $, then $ t_i^\star $ must satisfy all the constraints, which means that each $ t_i^\star $ necessarily forms a set $ F $ of feasible upper bounds for the constraints set $ C =  \lbrace x \in \bb{R} | \; t \geq x,\; t \geq x^2,\; t \in \bb{R} \rbrace $. Since $ F $ is a partially ordered set, the query for the least element (minimum) can be made. Moreover, by the <em>supremum property</em> $ F $ has an infimum (minimum) value because $ F $ is bounded below by $ C $.</p>
<p>The same idea can be applied easily for several constraint rules, for instance:
$$
\begin{equation}
\problemMinimizeSingle{\max(x^7 - x^6, x^5 - x^4, x^3 - x^2, x - 1)}{x \in \bb{R}}
\label{capsipex2}
\end{equation}
$$
can be reformulated into the equivalent one:
$$
\problemMinimizeSingle{t \in \bb{R}}{x^7 - x^6 \leq t,\; x^5 - x^4 \leq t,\; x^3 - x^2 \leq t,\; x - 1 \leq t,\; x \in \bb{R}}.
$$</p>
<p>And naturally, the same technique used in $ \eqref{capsipex1} $ and $ \eqref{capsipex2} $ can be applied for the semi-infinite programming problem in $ \eqref{capsipex3} $:
$$
\begin{equation}
\min_{x \in \bb{R}} \max_{i \in \bb{N}} g_i(x)
\label{capsipex3}
\end{equation}
$$
that is equivalent to the following problem:
$$
\min t \in \bb{R} \txt{s.t.} g_i(x) \leq t,\; i \in \bb{N}.
$$</p>
<p>Now, problem $ \eqref{chebyshevproblem} $ can be expressed without loss of generality <strong>[7:200]</strong> in the following terms 
$$
\displaylines{
  \min \quad f(x) := t \txt{with} \; x := (\tilde{x}, t)\;  \newline
  \txt{s.t.} \quad \tilde{x} \in K^{n-1},\; t \in \mathbb{R},\; g(x, w) := |d(w) - F(\tilde{x}, w)| \leq t ,\; w \in \Omega. \newline
}
$$
Note that the objective function and also the restriction $ g(\tilde{x}, w) $ are both linear. Finally, the previous problem can be reformulated again by splitting the absolute value restriction $ g(\tilde{x}, w) $ to get the kinks removed as in $ \eqref{capsipex1} $:</p>
<p>$$
\begin{equation}
\displaylines{
  \min \quad f(x) := t \txt{with} \; x := (\tilde{x}, t)\;  \newline
  \txt{s.t.} \quad \tilde{x} \in K_{n-1},\; t \in \mathbb{R} \newline
  \qquad \quad g_1(x, w) := d(w) - F(\tilde{x}, w) \leq t  \newline
  \qquad \quad g_2(x, w) := F(\tilde{x}, w) - d(w) \leq t  \newline
  \qquad \quad w \in \Omega.
}
\label{capsip}
\end{equation}
$$</p>
<p>This reformulation will allow to compute Chebyshev's approximations with computer software and frameworks available to solve semi-infinite programming problems. In next section the problem model components are defined.</p>
<h3 id="14-approximation-function-definition">1.4 Approximation function definition</h3>
<p>The problem shown in $ \eqref{capsip} $, requires that both <em>approximation function</em> $ F(x, w) $ and <em>target function</em> $ d(w) $ must be provided as problem's input. From now, the next <a href="#def-approximation-function">definition</a> will provide the function employed in numerical examples to approximate the example targets.</p>
<p><strong>Definition 1.4.1</strong> <em id="def-approximation-function">(Multivariate approximation)</em>   <br>
For given $ d \in \bb{N} \cup \lbrace 0 \rbrace $, let $ F $ be the <em>multivariate polynomial</em>:
$$ 
\begin{equation}
F(x, w; d) = x^Tz(w) = (x_1, ..., x_{n-1})^T(z_1(w), ..., z_{n-1}(w))
\label{multivariate-approx}
\end{equation}
$$
where $ x \in K_{n-1} $ is the polynomial coefficients tuple, $ w \in \Omega $, $ z: \Omega \mapsto K_{n-1} $ a vector function with $ \Omega \subseteq \mathbb{R}^{m} $, $ K_{n-1} \subseteq \mathbb{R}^{n-1} $, and each $ z_i $ the following monomial:
$$ 
z_i = w_1^{p_1} w_2^{p_2} ... w_{m}^{p_{m}},\; \sum_{j=0}^{m} p_j \leq d.
$$
with $ i = 1, ..., n-1 $ and $ p_j \in \bb{N} $. $ \quad \Box $</p>
<p>Note that the number of monomials $ k $ that will add up the polynomial is:
$$
k = \binom{m + d}{d}.
$$
Since each monomial has a coefficient and the factors involved in the dot product for $ \eqref{multivariate-approx} $ must be dimensional consistent, the coefficients tuple dimension $ dim(x) = k $, that is $ n - 1 = k $.</p>
<h3 id="15-problem-model-specification">1.5 Problem model specification</h3>
<p>Aiming to employ the SIP reformulation $ \eqref{capsip} $ within the SQP framework, here we define the problem model (<em>objective function</em>, <em>decision variables</em> and <em>constraints</em>) to feed the SQP method exposed in <a href="#2-sqp-method">section 2</a>. </p>
<p><strong>Definition 1.5.1</strong> <em id="objective-function">(Objective function)</em> <br>
Recalling the SIP minimization problem $ \eqref{capsip} $, the objective function for CAP is
$$
f(\tilde{x}, t) = t
$$ 
where $ f: K_{n-1} \times \bb{R} \mapsto \bb{R}. \quad \Box $</p>
<p><strong>Definition 1.5.2</strong> <em id="decision-variables">(Decision variables)</em> <br>
Recalling the SIP minimization problem $ \eqref{capsip} $, the decision variables for CAP are $ \tilde{x} $ and $ w $. $ \Box $</p>
<p><strong>Definition 1.5.3</strong> <em id="constraints">(Constraints)</em> <br>
Recalling the SIP minimization problem $ \eqref{capsip} $, the constraints for CAP are:
$$
  \displaylines{
    \tilde{x} \in K_{n-1}, \newline
    t \in \mathbb{R}, \newline
    w \in \Omega, \newline
    d(w) - F(\tilde{x}, w) \leq t  \newline
    F(\tilde{x}, w) - d(w) \leq t.
  }
$$
where the target function $ d(w) $ is a given preset example (see next <a href="#16-problem-instances">subsection</a>) and approximation function $ F $ is defined as in $ \eqref{multivariate-approx} $: $ F(\tilde{x}, w) := F(\tilde{x}, w; d) $. $ \Box $</p>
<h3 id="16-problem-instances">1.6 Problem instances</h3>
<p>In order to compare the results of this work with <a href="#ref4">[4]</a> and <a href="#ref8">[8]</a>, let $ d(w) $ be any of the following approximation targets:</p>
<p><strong id="example1">Example 1.6.1</strong> <br>
Refer to CAP-SIP $ \eqref{capsip} $ and let $ d(w) := d_1(w) $, $ w \in \Omega $ and $ F(x, w) $ as in $ \eqref{multivariate-approx} $:
$$
\displaylines{
  d_1(w) := d(w_1, w_2) = \log(w_1 + w_2)\sin(w_1) \newline
  w \in \Omega = [0, 1] \times [1, 2.5] \newline
  F(x, w; 2) = 
     x_1w_1^0w_2^0
    +x_2w_1^0w_2^1
    +x_3w_1^0w_2^2
    +x_4w_1^1w_2^0
    +x_5w_1^1w_2^1
    +x_6w_1^2w_2^0
}
$$</p>
<p><strong id="example2">Example 1.6.2</strong>  <br>
Refer to CAP-SIP $ \eqref{capsip} $ and let $ d(w) := d_2(w) $, $ w \in \Omega $ and $ F(x, w) $ as in $ \eqref{multivariate-approx} $:
$$
\displaylines{
  d_2(w) := d(w_1, w_2) = (1 + w_1)^{w_2} \newline
  w \in \Omega = [0, 1] \times [1, 2.5] \newline
  F(x, w; 2) = 
     x_1w_1^0w_2^0
    +x_2w_1^0w_2^1
    +x_3w_1^0w_2^2
    +x_4w_1^1w_2^0
    +x_5w_1^1w_2^1
    +x_6w_1^2w_2^0
}
$$</p>
<h2 id="2-sqp-method">2. SQP Method</h2>
<p>CAP will be computed with an open source software implementation of Sequential Programming Method (SQP). In particular the implementation provided by MATLAB will be used, that can be found in <em>fseminf</em> routine that belongs to the Optimization Package Extension. </p>
<p>Current section's aim is to explain the core gears of SQP method, that relies on the concepts of <em>Newton's Method</em> for polynomial root approximation, <em>Langrage multipliers</em> for local constraint optimization, <em>Kuhn-Karush-Tucker</em> optimality conditions and <em>Quadratic Programming</em>. Those methods and techniques requires that the objective functions and constraints must be smooth; this ensures a predictable algorithm behaviour because they are designed on top of the essence of <em>Calculus Theory</em>.</p>
<h3 id="21-newtons-method">2.1 Newton's Method</h3>
<p>The Newton's method is a numerical method that approximates the roots of a smooth function (i.e. $ x $ where $ f(x) $ vanishes). Since this is a necessary condition for a maximizer point, this method is employed intensively within the optimization theory.</p>
<p>Having the nonlinear uncostrained minimization problem $ \problemUnconstr $ one necessary condition for the optimal point $ x^\star $ is $ g(x^\star) = \nabla{f(x^\star)} = 0 $. That means we have a system of $ n $ non-linear equations that must be equal to $ 0 $ in order to minimize $ f(x) $.</p>
<p>The idea of the method is to employ a linear approximation for the point that should be the function's root. This is written as usual:
$$
\begin{equation}
g(x + h) = g(x) + g'(x)h + R(x, h)
\end{equation}
$$</p>
<p>Where $ h $ is a step vector from the point $ x $ that we want to approximate and $ R(x, h) $ is the remainder of this linear approximation. Then, with this approximation the root can be found as:
$$
\begin{equation}
\displaylines{
  g(x) + g'(x)h &amp;= 0 \newline
  g'(x)h &amp;= -g(x) \newline
  h &amp;= -\frac{g(x)}{g'(x)h} \newline
}
\end{equation}
$$</p>
<p>Since this result is an approximation to the true $ x $ that vanishes $ g(x) $, the value $ x^N = x + h $ can be employed as starting point for the next iteration. The method finishes when a precision threshold is met.</p>
<p>As pointed out by several authors <strong>[3]</strong> this is one of the most important techniques in numerical optimization because of its fast rate of convergence. In fact, some optimization books (<strong>[2]</strong>, <strong>[3]</strong>) devote at least one chapter to develop better convergence rates and to lease techinique's undesired behaviors.</p>
<p>As stated before, and for the following subsections, here we will only provide an overview of the core method. Further details can be found in the addressed references.</p>
<h3 id="22-lagrange-multipliers">2.2 Lagrange Multipliers</h3>
<p>This is one of the fundamental techniques of constrained optimization. Here the technique is reviewed incrementally, starting from one equality constraint, then with several equality constraints and finally with unequality constraints.</p>
<p>This method appears naturally when the unconstrained problem $ \min_{x \in \bb{R}} f(x) $ becomes constrained by one equality: 
$$
\displaylines {
  \min_{x \in \bb{R}^n} f(x) \txt{s.t.} g(x) = 0
}
$$ 
where $ f $ and $ g $ are real-valued smooth functions on a subset of $ \bb{R}^n $. At its core the method imposes a necessary condition to any critical point $ x^\star $, as stated in the following theorem. </p>
<p><strong>Theorem 2.2.1</strong> <em id="thrm-lagrange-mult-uni">(Lagrange Multipliers)</em>  <br>
Let $ f: K^n \mapsto \bb{R} $ and $ g: K^n \mapsto \bb{R} $ be $ C^1 $ real functions, $ K^n \subseteq \bb{R}^n $, $ x^\star \in K $, $ g(x^\star) = c $, $ S = \lbrace x \in \bb{R} \;|\; g(x) = c \rbrace $ (i.e. the level set) and $ \nabla g(x^\star) ≠ 0 $. If $ f|S $ ($ f $ restricted to $ S $) has an optimal value at $ x^\star $, then there is a real number $ \lambda $ such that
$$
  \nabla f(x^\star) = \lambda \nabla g(x^\star). \quad \Box
  \label{lagrangeoptcriteria}
$$</p>
<p>Seemingly, if there are several equality constraints, for the problem:
$$
  \min_{x \in \bb{R}^n} f(x) \txt{s.t} g_i(x) = 0, \; i \in \cc{E}
$$
where $ f $ and $ g $ are real-valued smooth functions on a subset of $ \bb{R}^n $ and  $ \cc{E} $ is a finite set indices, the previous theorem can be extended as follows.</p>
<p><strong>Theorem 2.2.2</strong> <em id="thrm-lagrange-mult">(Lagrange Multipliers)</em>  <br>
If $ f $ has a maximum or minimum at $ x^\star $ on $ S = \lbrace x \in \bb{R} \;|\; g_i(x) = 0, i \in \cc{E} \rbrace $ and the vectors $ \nabla g_i $ for $ i \in \cc{E} $ are linearly independent, then must exist constants $ \lambda_i $  such that:
$$
\nabla f(x^\star) = \sum_{i \in \cc{E}} \lambda_{i} \nabla g_i(x^\star). \quad \Box
$$</p>
<p>Naturally, Lagrange multipliers can be employed to solve optimization problems that also involves unequality constraints, such as this general formulation:
$$
\min_{x \in \bb{R}}f(x) \txt{s.t.}
\left\lbrace\displaylines{
  \quad g_i(x) =    0, \; i \in \cc{E} \quad \newline
  \quad g_i(x) \leq 0, \; i \in \cc{I} \quad
}\right\rbrace
$$
where $ f $ and $ g_i $ are real-valued smooth functions on a subset of $ \bb{R}^n $ for $ i \in \cc{E} \cup \cc{I}$, and $ \cc{E}$ and $ \cc{I} $ are finite set of indices for equality and inequality constraints respectively.</p>
<!-- TODO: Write down the strategy to optimize regions using lagrange multipliers -->

<p>The equality criteria stated in previous theorem is referred as <em>Lagrangian function</em>:
$$
\begin{equation}
  \cc{L}(x, \lambda) = f(x) - \sum_{i \in \cc{E}} \lambda_{i} \nabla g_i(x)
\end{equation}
$$</p>
<h3 id="23-kkt-optimality-conditions">2.3 KKT optimality conditions</h3>
<p>Known as <em>First-Order Necessary Conditions</em>, are conditions concerned to the gradients of a local solution $ x^\star $.</p>
<p><strong id="theorem-kkt">Theorem 2.3.1</strong>
<em>Let $ F(x, w) $ an approximation function and let x be an arbirary number in $ \Omega $. Then there exists one element in $ F_i $ such that x &lt; 3</em></p>
<h3 id="24-quadratic-programming">2.4 Quadratic programming</h3>
<p>Es un método de aproximación local</p>
<h3 id="25-sequential-quadratic-programming-method-sqp">2.5 Sequential Quadratic Programming method SQP</h3>
<p>Existen varios métodos SQP, el IQP y el EQP. Actualmente la librería emplea un método...</p>
<h2 id="3-examples-computation">3. Examples computation</h2>
<h2 id="4-source-code-revision">4. Source code revision</h2>
<p>In this section we will examine the source code of <em>fseminf</em> routine. Having the 
knoledge of SQP Methods, we will point out what ideas are applied and in which 
parts.
After defining the approximation problem in terms of SIP, only left to pour the functions $ F(x, w) $ and $ d(w) $ to $ \eqref{chebyshevproblem} $. </p>
<h2 id="todo">TODO</h2>
<ul>
<li>Change 'continuous' by 'smooth', or belongs to C^i, because 'continuos' does not  mean 'diffrenciable'.</li>
<li>Expand the idea of necessary and sufficient conditions, to expand the Lagrange multipliers method to region constraints.</li>
<li>Add the examples to the end of the first section.</li>
</ul>
<h2 id="referencias">Referencias</h2>
<p><strong>[1]</strong> MathWorks - https://www.mathworks.com/help/optim/ug/fseminf.html  <br>
<strong>[2]</strong> Numerical Optimization Jorge Nocedal, Stephen Wright - (2006)  <br>
<strong>[3]</strong> Numerical optimization theoretical and practical - J. Bonnans, J. Gilbert, C. Lemarechal, C. Sagastizábal - (2006) <br>
<strong id="ref4">[4]</strong> Reemtsen R., Discretizations Methods for the Solutions of Semi-
In nite Programming Problems, J. Optim. Theory Appl, 71 (1991),
pp. 85-103.  <br>
<strong>[6]</strong> Nocedal <br>
<strong>[7]</strong> Moradito
<strong ref8>[8]</strong> Carlos Gamboa</p>
    </div>
    <div class="column-side">
    </div>
  </div>

  </body>
  <script>
    /***********************************************************************
     * DISPLAY CONFIGURATION                                               *
     ***********************************************************************/
    // var ismobile = navigator.userAgent.match(/(iPad)|(iPhone)|(iPod)|(android)|(webOS)/i);
    let sides = document.getElementsByClassName("column-side");
    let content = document.getElementsByClassName("column-content")[0];

    var w = window.innerWidth;
    var h = window.innerHeight; 
    if (w/h < 1) {
      sides[0].classList.remove("column-side");
      sides[0].classList.remove("column-side");
      content.classList.remove("column-content");
    }
  </script>
</html>last update: 2022-08-23 14:25:30.186940